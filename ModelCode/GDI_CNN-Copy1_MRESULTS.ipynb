{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(1)\n",
    "rn.seed(1)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv1D,MaxPooling1D,Conv2D,MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TerminateOnNaN\n",
    "import keras.regularizers\n",
    "import scipy\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import linregress\n",
    "from scipy import interpolate\n",
    "from scipy import signal\n",
    "#import cPickle as pickle\n",
    "#from video_process_utils import *\n",
    "import collections\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "#os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ImportImage(filepath):\n",
    "    #print(filepath)\n",
    "    img = Image.open(filepath)\n",
    "    return np.array(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagepaths_labels = pd.read_csv('./data/imagepath_gdi_10.csv')\n",
    "msk = np.random.rand(len(imagepaths_labels)) < 0.8\n",
    "train=imagepaths_labels[msk]\n",
    "validation=imagepaths_labels[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_imgs = np.array([ImportImage(img) for img in train['image_path'].values])\n",
    "train_labels = np.array([label for label in train['labels_norm'].values])\n",
    "validation_imgs = np.array([ImportImage(img) for img in validation['image_path'].values])\n",
    "validation_labels = np.array([label for label in validation['labels_norm'].values])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images shape :  (311, 480, 640, 3)\n",
      "train labels shape :  (311,)\n",
      "validation images shape :  (85, 480, 640, 3)\n",
      "validation labels shape :  (85,)\n"
     ]
    }
   ],
   "source": [
    "print('train images shape : ',train_imgs.shape)\n",
    "print('train labels shape : ',train_labels.shape)\n",
    "print('validation images shape : ',validation_imgs.shape)\n",
    "print('validation labels shape : ',validation_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image in enumerate(train_imgs):\n",
    "    if (np.count_nonzero(image)<1):\n",
    "        print('all zeros at : ', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image in enumerate(validation_imgs):\n",
    "    if (np.count_nonzero(image)<1):\n",
    "        print('all zeros at : ', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(initial_lrate,epochs_drop,drop_factor):\n",
    "    def step_decay_fcn(epoch):\n",
    "        return initial_lrate * math.pow(drop_factor, math.floor((1+epoch)/epochs_drop))\n",
    "    return step_decay_fcn\n",
    "\n",
    "checkpoint_folder = \"./cnn_checkpoints_gdicnn\"\n",
    "batch_size  = 16\n",
    "num_classes = len(train_labels)\n",
    "epochs = 10\n",
    "epochs_drop,drop_factor = (10,0.8)\n",
    "input_shape = train_imgs[0].shape\n",
    "kernel_size = 8\n",
    "conv_dim = 16\n",
    "initial_lrate = 0.001\n",
    "dropout_amount = 0.5\n",
    "l2_lambda = 10**(-3.5)\n",
    "reg = keras.regularizers.l2(l2_lambda)\n",
    "last_layer_dim=18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(conv_dim, kernel_size=kernel_size, input_shape=input_shape, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "#model.add(Dropout(dropout_amount))\n",
    "model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "#model.add(Dropout(dropout_amount))\n",
    "model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=3))\n",
    "#model.add(Dropout(dropout_amount))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(last_layer_dim,activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 311 samples, validate on 85 samples\n",
      "Epoch 1/10\n"
     ]
    }
   ],
   "source": [
    "train_model = True\n",
    "\n",
    "if not os.path.exists(checkpoint_folder):\n",
    "    os.makedirs(checkpoint_folder)\n",
    "\n",
    "filepath=checkpoint_folder+\"/weights-{epoch:02d}.hdf5\"\n",
    "if train_model:\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = RMSprop(lr=0.0,rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='mse',metrics=['accuracy'],optimizer=opt)\n",
    "    #model.compile(loss='mse', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    checkpoint = \\\n",
    "        ModelCheckpoint(filepath, verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "    lr = LearningRateScheduler(step_decay(initial_lrate,epochs_drop,drop_factor))\n",
    "\n",
    "    history = model.fit(train_imgs, train_labels,callbacks=[checkpoint,lr,TerminateOnNaN()],\n",
    "              validation_data=(validation_imgs,validation_labels),\n",
    "              batch_size=batch_size, epochs=epochs,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size  = 18\n",
    "dropout_amount = 0.1\n",
    "checkpoint_folder = \"./cnn_checkpoints_gdicnn_loss_sce\"\n",
    "reg = keras.regularizers.l2(l2_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.np_utils import to_categorical\n",
    "train_categorical_labels = to_categorical(train_labels, num_classes=None)\n",
    "validation_categorical_labels = to_categorical(validation_labels, num_classes=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_categorical_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_categorical_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(conv_dim, kernel_size=kernel_size, input_shape=input_shape, padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(dropout_amount))\n",
    "model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=2))\n",
    "model.add(Dropout(dropout_amount))\n",
    "model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=3))\n",
    "model.add(Dropout(dropout_amount))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(last_layer_dim,activation='relu'))\n",
    "model.add(Dense(18, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3834 samples, validate on 951 samples\n",
      "Epoch 1/10\n",
      "3834/3834 [==============================] - 310s 81ms/step - loss: 3.0017 - acc: 0.1403 - val_loss: 2.8217 - val_acc: 0.1314\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 2.82170, saving model to ./cnn_checkpoints_gdicnn_loss_sce/weights-01.hdf5\n",
      "Epoch 2/10\n",
      "3834/3834 [==============================] - 304s 79ms/step - loss: 2.7486 - acc: 0.1547 - val_loss: 2.7242 - val_acc: 0.1535\n",
      "\n",
      "Epoch 00002: val_loss improved from 2.82170 to 2.72421, saving model to ./cnn_checkpoints_gdicnn_loss_sce/weights-02.hdf5\n",
      "Epoch 3/10\n",
      "3834/3834 [==============================] - 305s 80ms/step - loss: 2.6770 - acc: 0.1638 - val_loss: 3.1110 - val_acc: 0.1493\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 2.72421\n",
      "Epoch 4/10\n",
      "3834/3834 [==============================] - 309s 81ms/step - loss: 2.6561 - acc: 0.1596 - val_loss: 2.6370 - val_acc: 0.1514\n",
      "\n",
      "Epoch 00004: val_loss improved from 2.72421 to 2.63705, saving model to ./cnn_checkpoints_gdicnn_loss_sce/weights-04.hdf5\n",
      "Epoch 5/10\n",
      "3834/3834 [==============================] - 305s 79ms/step - loss: 2.5843 - acc: 0.1630 - val_loss: 2.5514 - val_acc: 0.1630\n",
      "\n",
      "Epoch 00005: val_loss improved from 2.63705 to 2.55143, saving model to ./cnn_checkpoints_gdicnn_loss_sce/weights-05.hdf5\n",
      "Epoch 6/10\n",
      "3834/3834 [==============================] - 309s 81ms/step - loss: 2.5332 - acc: 0.1695 - val_loss: 2.5611 - val_acc: 0.1651\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 2.55143\n",
      "Epoch 7/10\n",
      "3834/3834 [==============================] - 301s 78ms/step - loss: 2.5145 - acc: 0.1690 - val_loss: 2.5213 - val_acc: 0.1630\n",
      "\n",
      "Epoch 00007: val_loss improved from 2.55143 to 2.52126, saving model to ./cnn_checkpoints_gdicnn_loss_sce/weights-07.hdf5\n",
      "Epoch 8/10\n",
      "3834/3834 [==============================] - 301s 78ms/step - loss: 2.4823 - acc: 0.1745 - val_loss: 2.4904 - val_acc: 0.1682\n",
      "\n",
      "Epoch 00008: val_loss improved from 2.52126 to 2.49038, saving model to ./cnn_checkpoints_gdicnn_loss_sce/weights-08.hdf5\n",
      "Epoch 9/10\n",
      "3834/3834 [==============================] - 303s 79ms/step - loss: 2.4556 - acc: 0.1821 - val_loss: 2.4685 - val_acc: 0.1756\n",
      "\n",
      "Epoch 00009: val_loss improved from 2.49038 to 2.46849, saving model to ./cnn_checkpoints_gdicnn_loss_sce/weights-09.hdf5\n",
      "Epoch 10/10\n",
      "3834/3834 [==============================] - 308s 80ms/step - loss: 2.4335 - acc: 0.1841 - val_loss: 2.4295 - val_acc: 0.1924\n",
      "\n",
      "Epoch 00010: val_loss improved from 2.46849 to 2.42946, saving model to ./cnn_checkpoints_gdicnn_loss_sce/weights-10.hdf5\n"
     ]
    }
   ],
   "source": [
    "train_model = True\n",
    "\n",
    "if not os.path.exists(checkpoint_folder):\n",
    "    os.makedirs(checkpoint_folder)\n",
    "\n",
    "filepath=checkpoint_folder+\"/weights-{epoch:02d}.hdf5\"\n",
    "if train_model:\n",
    "\n",
    "    opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "    #opt = RMSprop(lr=0.0,rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    #model.compile(loss=keras.losses.sparse_categorical_crossentropy,metrics=['accuracy'],optimizer=opt)\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,metrics=['accuracy'],optimizer=opt)\n",
    "    #model.compile(loss='mse', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    checkpoint = \\\n",
    "        ModelCheckpoint(filepath, verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "    lr = LearningRateScheduler(step_decay(initial_lrate,epochs_drop,drop_factor))\n",
    "\n",
    "    history = model.fit(train_imgs, train_categorical_labels,callbacks=[checkpoint,lr,TerminateOnNaN()],\n",
    "              validation_data=(validation_imgs,validation_categorical_labels),\n",
    "              batch_size=batch_size, epochs=epochs,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
