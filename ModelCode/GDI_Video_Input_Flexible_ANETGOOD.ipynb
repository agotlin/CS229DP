{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specs for running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_to_run_script = 'local' # 'Sherlock', 'local'\n",
    "spatial_component = 'ANet'#'vgg16' # 'vgg16', 'ANet'\n",
    "temporal_component = 'lstm' # '1dconv_v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(1)\n",
    "rn.seed(1)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv1D,MaxPooling1D,Conv2D,MaxPooling2D\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TerminateOnNaN\n",
    "import keras.regularizers\n",
    "import scipy\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import linregress\n",
    "from scipy import interpolate\n",
    "from scipy import signal\n",
    "import collections\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from time import time\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to load data\n",
    "\n",
    "def ImportImage(filepath):\n",
    "    #Check if file path exists\n",
    "    img = Image.open(filepath)\n",
    "    return np.array(img)\n",
    "\n",
    "def windows(data, size, sample_stride): # define time windows to create each training example\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += sample_stride\n",
    "\n",
    "def hollywood(data_full, input_window_size, sample_stride): # make video files \n",
    "    list_of_examples = []\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data_full['rownum'], input_window_size, sample_stride):   \n",
    "        if(end < data_full.shape[0] and # we are not at the end of the total frames\n",
    "            len(data_full['rownum'][start:end]) == input_window_size and  # not sure\n",
    "            int(imagepaths_wlabels['image_path'][start][21:29])==int(imagepaths_wlabels['image_path'][end-1][21:29])):  # the end patientID = start patiendID\n",
    "            # Pull ten frames, crop the images, stack them\n",
    "            ten_frames = np.array([ImportImage(img) for img in data_full['image_path'][start:end].values])\n",
    "            if spatial_component == 'vgg16': # crop images to fit into vgg16 model architecture\n",
    "                ten_frames_crops_res = ten_frames[:,16::2, 50:274, :]\n",
    "                ten_frames_crops_res = ten_frames_crops_res[:,0:224, :, :]\n",
    "                list_of_examples.append(ten_frames_crops_res)\n",
    "            else:\n",
    "                list_of_examples.append(ten_frames)\n",
    "            labels = np.append(labels,data_full['labels'][start])          \n",
    "            \n",
    "    return np.array(list_of_examples), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in path file\n",
    "if machine_to_run_script == 'local':\n",
    "    imagepaths_wlabels = pd.read_csv('./data/test/labels/imagepath_gdi_50.csv') # TO USE REGRESSION\n",
    "elif machine_to_run_script == 'Sherlock':\n",
    "    imagepaths_wlabels = pd.read_csv('.?????/imagepath_gdi_10_frames.csv') # TO USE REGRESSION\n",
    "\n",
    "# Create video outputs with labels\n",
    "frames_per_video = 10\n",
    "stride_per_video = 5\n",
    "videos, labels = hollywood(imagepaths_wlabels, frames_per_video, stride_per_video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "videos_normalized = videos #(videos - videos.mean())/videos.std()\n",
    "\n",
    "# Split data into train and validation\n",
    "msk = np.random.rand(len(videos_normalized)) < 0.8\n",
    "train_videos=videos_normalized[msk]\n",
    "train_videos_labels=labels[msk]\n",
    "validation_videos=videos_normalized[~msk]\n",
    "validation_videos_labels=labels[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3653, 10, 480, 640, 3)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "videos.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images shape :  (2920, 10, 480, 640, 3)\n",
      "train labels shape :  (2920,)\n",
      "validation images shape :  (733, 10, 480, 640, 3)\n",
      "validation labels shape :  (733,)\n"
     ]
    }
   ],
   "source": [
    "# Check expected shapes and whether data is populated with zeros\n",
    "\n",
    "print('train images shape : ',train_videos.shape)\n",
    "print('train labels shape : ',train_videos_labels.shape)\n",
    "print('validation images shape : ',validation_videos.shape)\n",
    "print('validation labels shape : ',validation_videos_labels.shape)\n",
    "#print(train_videos[0,1,10:15,15:20,1]) # show a random rectangle of one channel of the the image\n",
    "\n",
    "# Check if any images are zeros\n",
    "for i, video in enumerate(train_videos):\n",
    "    if (np.count_nonzero(video)<1):\n",
    "        print('all zeros at : ', i)\n",
    "for i, video in enumerate(validation_videos):\n",
    "    if (np.count_nonzero(video)<1):\n",
    "        print('all zeros at : ', i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Component of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "\n",
    "checkpoint_folder = \"./cnn_checkpoints_gdicnn_lstm\"\n",
    "epochs = 100\n",
    "epochs_drop,drop_factor = (10,0.95)\n",
    "batch_size  = 4\n",
    "video_shape = train_videos[0].shape\n",
    "frame_shape = train_videos[0][0].shape\n",
    "kernel_size = 8\n",
    "conv_dim = 8 # number of kernels\n",
    "initial_lrate = 0.001\n",
    "dropout_amount = 0.5\n",
    "l2_lambda = 10**(-3.5)\n",
    "reg = keras.regularizers.l2(l2_lambda)\n",
    "\n",
    "def step_decay(initial_lrate,epochs_drop,drop_factor):\n",
    "    def step_decay_fcn(epoch):\n",
    "        return initial_lrate * math.pow(drop_factor, math.floor((1+epoch)/epochs_drop))\n",
    "    return step_decay_fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 480, 640, 3)\n",
      "(480, 640, 3)\n"
     ]
    }
   ],
   "source": [
    "print(video_shape)\n",
    "print(frame_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "246"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spatial_component == 'vgg16':\n",
    "\n",
    "    # Configure VGG Model:\n",
    "    vgg16_model = keras.applications.vgg16.VGG16() # Download Model\n",
    "    type(vgg16_model) #This is a Keras Functional API need to convert to sequential\n",
    "    Frame_model = Sequential() #Iterate over the functional layers and add it as a stack\n",
    "    for layer in vgg16_model.layers:\n",
    "        Frame_model.add(layer)\n",
    "\n",
    "    # Remove last layer of VGG:\n",
    "    Frame_model.layers.pop()\n",
    "\n",
    "    # Fix the VGG model\n",
    "    for layer in Frame_model.layers: #Since the model is already trained with certain weights, we dont want to change it. Let it be the same\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add a Dense layer to VGG\n",
    "    Frame_model.add(Dense(32, activation='relu')) # Add a connected layer\n",
    "    \n",
    "\n",
    "elif spatial_component == 'dummy':\n",
    "    \n",
    "    Frame_model = Sequential()\n",
    "    Frame_model.add(Conv2D(conv_dim, kernel_size=kernel_size, input_shape=frame_shape, padding='valid', strides = 5))\n",
    "    Frame_model.add(Activation('relu'))\n",
    "    Frame_model.add(BatchNormalization())\n",
    "    Frame_model.add(Flatten())\n",
    "    Frame_model.add(Dense(18,activation='relu'))\n",
    "\n",
    "elif spatial_component == 'ANet':\n",
    "\n",
    "    Frame_model = Sequential()\n",
    "    Frame_model.add(Conv2D(conv_dim, kernel_size=kernel_size, input_shape=frame_shape, padding='same'))\n",
    "    Frame_model.add(Activation('relu'))\n",
    "    Frame_model.add(BatchNormalization())\n",
    "    Frame_model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same'))\n",
    "    Frame_model.add(Activation('relu'))\n",
    "    Frame_model.add(BatchNormalization())\n",
    "    Frame_model.add(MaxPooling2D(pool_size=2))\n",
    "    Frame_model.add(Dropout(dropout_amount))\n",
    "    Frame_model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "    Frame_model.add(Activation('relu'))\n",
    "    Frame_model.add(BatchNormalization())\n",
    "    Frame_model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "    Frame_model.add(Activation('relu'))\n",
    "    Frame_model.add(BatchNormalization())\n",
    "    Frame_model.add(MaxPooling2D(pool_size=2))\n",
    "    Frame_model.add(Dropout(dropout_amount))\n",
    "    Frame_model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "    Frame_model.add(Activation('relu'))\n",
    "    Frame_model.add(BatchNormalization())\n",
    "    Frame_model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "    Frame_model.add(Activation('relu'))\n",
    "    Frame_model.add(BatchNormalization())\n",
    "    Frame_model.add(MaxPooling2D(pool_size=3))\n",
    "    Frame_model.add(Dropout(dropout_amount))\n",
    "    Frame_model.add(Flatten())\n",
    "    Frame_model.add(Dense(16,activation='relu'))\n",
    "    #Frame_model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_12 (InputLayer)        (None, 10, 480, 640, 3)   0         \n",
      "_________________________________________________________________\n",
      "time_distributed_12 (TimeDis (None, 10, 16)            293632    \n",
      "_________________________________________________________________\n",
      "lstm_12 (LSTM)               (None, 256)               279552    \n",
      "_________________________________________________________________\n",
      "dense_23 (Dense)             (None, 1)                 257       \n",
      "=================================================================\n",
      "Total params: 573,441\n",
      "Trainable params: 573,345\n",
      "Non-trainable params: 96\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "if temporal_component == 'lstm':\n",
    "    \n",
    "    # Declare video_model and apply frame_model to each frame\n",
    "    video_input = Input(shape=video_shape) # usually 10 frames that are 224x224x3 each\n",
    "    encoded_frame_sequence = TimeDistributed(Frame_model)(video_input) # Run frame_model on each frame\n",
    "\n",
    "    # Add LSTM                  \n",
    "    encoded_video = LSTM(256)(encoded_frame_sequence)  # the output will be a vector\n",
    "\n",
    "    # Add a linear layer for output of video model\n",
    "    output = Dense(1, activation='linear')(encoded_video)\n",
    "\n",
    "    # Configure video_model\n",
    "    video_model = Model(inputs=video_input, outputs=output)   \n",
    "    \n",
    "    video_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model and Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "if not os.path.exists(checkpoint_folder):\n",
    "    os.makedirs(checkpoint_folder)\n",
    "\n",
    "filepath=checkpoint_folder+\"/weights-{epoch:02d}.hdf5\"\n",
    "\n",
    "# Create optimizer\n",
    "opt = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Declare optimizer, lsos function, and reporting metrics\n",
    "video_model.compile(loss='mse',metrics=['mae'],optimizer=opt)\n",
    "\n",
    "# Create checkpoints to keep track of weights\n",
    "checkpoint = \\\n",
    "    ModelCheckpoint(filepath, verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# Define learning rate function\n",
    "lr = LearningRateScheduler(step_decay(initial_lrate,epochs_drop,drop_factor))\n",
    "\n",
    "#tensorboard\n",
    "tensorboard = keras.callbacks.TensorBoard(log_dir='./tensorboard-logs/lstm/{}'.format(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2920 samples, validate on 733 samples\n",
      "Epoch 1/100\n",
      "2920/2920 [==============================] - 1397s 478ms/step - loss: 592.2477 - mean_absolute_error: 17.6592 - val_loss: 137.7064 - val_mean_absolute_error: 9.3150\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 137.70637, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-01.hdf5\n",
      "Epoch 2/100\n",
      "2920/2920 [==============================] - 1386s 475ms/step - loss: 135.2705 - mean_absolute_error: 9.2726 - val_loss: 125.4086 - val_mean_absolute_error: 8.8754\n",
      "\n",
      "Epoch 00002: val_loss improved from 137.70637 to 125.40862, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-02.hdf5\n",
      "Epoch 3/100\n",
      "2920/2920 [==============================] - 1394s 478ms/step - loss: 124.7991 - mean_absolute_error: 8.8763 - val_loss: 136.1589 - val_mean_absolute_error: 9.0884\n",
      "\n",
      "Epoch 00003: val_loss did not improve from 125.40862\n",
      "Epoch 4/100\n",
      "2920/2920 [==============================] - 1386s 474ms/step - loss: 122.3606 - mean_absolute_error: 8.7705 - val_loss: 132.4580 - val_mean_absolute_error: 8.8680\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 125.40862\n",
      "Epoch 5/100\n",
      "2920/2920 [==============================] - 1403s 480ms/step - loss: 117.8368 - mean_absolute_error: 8.5768 - val_loss: 123.6830 - val_mean_absolute_error: 8.6598\n",
      "\n",
      "Epoch 00005: val_loss improved from 125.40862 to 123.68305, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-05.hdf5\n",
      "Epoch 6/100\n",
      "2920/2920 [==============================] - 1739s 596ms/step - loss: 112.2550 - mean_absolute_error: 8.3816 - val_loss: 112.8926 - val_mean_absolute_error: 8.3308\n",
      "\n",
      "Epoch 00006: val_loss improved from 123.68305 to 112.89255, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-06.hdf5\n",
      "Epoch 7/100\n",
      "2920/2920 [==============================] - 1938s 664ms/step - loss: 110.6969 - mean_absolute_error: 8.3339 - val_loss: 119.9804 - val_mean_absolute_error: 8.4518\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 112.89255\n",
      "Epoch 8/100\n",
      "2920/2920 [==============================] - 1403s 480ms/step - loss: 109.3426 - mean_absolute_error: 8.2039 - val_loss: 109.9567 - val_mean_absolute_error: 8.1602\n",
      "\n",
      "Epoch 00008: val_loss improved from 112.89255 to 109.95673, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-08.hdf5\n",
      "Epoch 9/100\n",
      "2920/2920 [==============================] - 1385s 474ms/step - loss: 104.6410 - mean_absolute_error: 7.9915 - val_loss: 110.1366 - val_mean_absolute_error: 8.1456\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 109.95673\n",
      "Epoch 10/100\n",
      "2920/2920 [==============================] - 1375s 471ms/step - loss: 100.6630 - mean_absolute_error: 7.8480 - val_loss: 109.3144 - val_mean_absolute_error: 8.1236\n",
      "\n",
      "Epoch 00010: val_loss improved from 109.95673 to 109.31439, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-10.hdf5\n",
      "Epoch 11/100\n",
      "2920/2920 [==============================] - 1363s 467ms/step - loss: 107.3465 - mean_absolute_error: 8.0709 - val_loss: 103.1479 - val_mean_absolute_error: 7.8932\n",
      "\n",
      "Epoch 00011: val_loss improved from 109.31439 to 103.14793, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-11.hdf5\n",
      "Epoch 12/100\n",
      "2920/2920 [==============================] - 1362s 466ms/step - loss: 98.4768 - mean_absolute_error: 7.7367 - val_loss: 104.5743 - val_mean_absolute_error: 7.9160\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 103.14793\n",
      "Epoch 13/100\n",
      "2920/2920 [==============================] - 1362s 467ms/step - loss: 92.2858 - mean_absolute_error: 7.4622 - val_loss: 96.7438 - val_mean_absolute_error: 7.6255\n",
      "\n",
      "Epoch 00013: val_loss improved from 103.14793 to 96.74382, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-13.hdf5\n",
      "Epoch 14/100\n",
      "2920/2920 [==============================] - 1364s 467ms/step - loss: 86.9523 - mean_absolute_error: 7.2038 - val_loss: 90.0345 - val_mean_absolute_error: 7.3267\n",
      "\n",
      "Epoch 00014: val_loss improved from 96.74382 to 90.03450, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-14.hdf5\n",
      "Epoch 15/100\n",
      "2920/2920 [==============================] - 1362s 466ms/step - loss: 83.6484 - mean_absolute_error: 7.0477 - val_loss: 84.4517 - val_mean_absolute_error: 7.1461\n",
      "\n",
      "Epoch 00015: val_loss improved from 90.03450 to 84.45168, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-15.hdf5\n",
      "Epoch 16/100\n",
      "2920/2920 [==============================] - 1362s 466ms/step - loss: 79.2758 - mean_absolute_error: 6.8107 - val_loss: 86.7365 - val_mean_absolute_error: 7.2382\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 84.45168\n",
      "Epoch 17/100\n",
      "2920/2920 [==============================] - 1363s 467ms/step - loss: 72.8997 - mean_absolute_error: 6.5504 - val_loss: 76.7911 - val_mean_absolute_error: 6.7905\n",
      "\n",
      "Epoch 00017: val_loss improved from 84.45168 to 76.79106, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-17.hdf5\n",
      "Epoch 18/100\n",
      "2920/2920 [==============================] - 1363s 467ms/step - loss: 70.0709 - mean_absolute_error: 6.4136 - val_loss: 79.3164 - val_mean_absolute_error: 6.8704\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 76.79106\n",
      "Epoch 19/100\n",
      "2920/2920 [==============================] - 1363s 467ms/step - loss: 65.7929 - mean_absolute_error: 6.1712 - val_loss: 85.7204 - val_mean_absolute_error: 7.2215\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 76.79106\n",
      "Epoch 20/100\n",
      "2920/2920 [==============================] - 1363s 467ms/step - loss: 63.6363 - mean_absolute_error: 6.1008 - val_loss: 71.9788 - val_mean_absolute_error: 6.6175\n",
      "\n",
      "Epoch 00020: val_loss improved from 76.79106 to 71.97878, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-20.hdf5\n",
      "Epoch 21/100\n",
      "2920/2920 [==============================] - 1362s 467ms/step - loss: 54.6734 - mean_absolute_error: 5.6322 - val_loss: 66.7090 - val_mean_absolute_error: 6.2401\n",
      "\n",
      "Epoch 00021: val_loss improved from 71.97878 to 66.70898, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-21.hdf5\n",
      "Epoch 22/100\n",
      "2920/2920 [==============================] - 1364s 467ms/step - loss: 50.4998 - mean_absolute_error: 5.3919 - val_loss: 60.9073 - val_mean_absolute_error: 5.9934\n",
      "\n",
      "Epoch 00022: val_loss improved from 66.70898 to 60.90730, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-22.hdf5\n",
      "Epoch 23/100\n",
      "2920/2920 [==============================] - 1363s 467ms/step - loss: 46.9020 - mean_absolute_error: 5.1933 - val_loss: 54.6737 - val_mean_absolute_error: 5.6374\n",
      "\n",
      "Epoch 00023: val_loss improved from 60.90730 to 54.67373, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-23.hdf5\n",
      "Epoch 24/100\n",
      "2920/2920 [==============================] - 1362s 467ms/step - loss: 43.0199 - mean_absolute_error: 4.9511 - val_loss: 54.1120 - val_mean_absolute_error: 5.5108\n",
      "\n",
      "Epoch 00024: val_loss improved from 54.67373 to 54.11203, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-24.hdf5\n",
      "Epoch 25/100\n",
      "2920/2920 [==============================] - 1363s 467ms/step - loss: 38.6049 - mean_absolute_error: 4.6737 - val_loss: 50.3335 - val_mean_absolute_error: 5.4401\n",
      "\n",
      "Epoch 00025: val_loss improved from 54.11203 to 50.33347, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-25.hdf5\n",
      "Epoch 26/100\n",
      "2920/2920 [==============================] - 1362s 467ms/step - loss: 34.7545 - mean_absolute_error: 4.4160 - val_loss: 49.9493 - val_mean_absolute_error: 5.3370\n",
      "\n",
      "Epoch 00026: val_loss improved from 50.33347 to 49.94928, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-26.hdf5\n",
      "Epoch 27/100\n",
      "2920/2920 [==============================] - 1363s 467ms/step - loss: 29.6121 - mean_absolute_error: 4.0749 - val_loss: 49.4679 - val_mean_absolute_error: 5.2347\n",
      "\n",
      "Epoch 00027: val_loss improved from 49.94928 to 49.46793, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-27.hdf5\n",
      "Epoch 28/100\n",
      "2920/2920 [==============================] - 1363s 467ms/step - loss: 26.1543 - mean_absolute_error: 3.7973 - val_loss: 43.0928 - val_mean_absolute_error: 4.9820\n",
      "\n",
      "Epoch 00028: val_loss improved from 49.46793 to 43.09281, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-28.hdf5\n",
      "Epoch 29/100\n",
      "2920/2920 [==============================] - 1363s 467ms/step - loss: 27.4053 - mean_absolute_error: 3.8765 - val_loss: 44.6487 - val_mean_absolute_error: 5.0696\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 43.09281\n",
      "Epoch 30/100\n",
      "2920/2920 [==============================] - 1362s 466ms/step - loss: 23.4068 - mean_absolute_error: 3.6603 - val_loss: 39.9709 - val_mean_absolute_error: 4.7796\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00030: val_loss improved from 43.09281 to 39.97086, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-30.hdf5\n",
      "Epoch 31/100\n",
      "2920/2920 [==============================] - 1361s 466ms/step - loss: 18.9602 - mean_absolute_error: 3.2607 - val_loss: 35.3203 - val_mean_absolute_error: 4.4732\n",
      "\n",
      "Epoch 00031: val_loss improved from 39.97086 to 35.32033, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-31.hdf5\n",
      "Epoch 32/100\n",
      "2920/2920 [==============================] - 1362s 466ms/step - loss: 17.0692 - mean_absolute_error: 3.0876 - val_loss: 44.9152 - val_mean_absolute_error: 5.0138\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 35.32033\n",
      "Epoch 33/100\n",
      "2920/2920 [==============================] - 1361s 466ms/step - loss: 16.6399 - mean_absolute_error: 3.0080 - val_loss: 34.6314 - val_mean_absolute_error: 4.3780\n",
      "\n",
      "Epoch 00033: val_loss improved from 35.32033 to 34.63143, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-33.hdf5\n",
      "Epoch 34/100\n",
      "2920/2920 [==============================] - 1361s 466ms/step - loss: 14.5734 - mean_absolute_error: 2.8407 - val_loss: 33.0808 - val_mean_absolute_error: 4.2950\n",
      "\n",
      "Epoch 00034: val_loss improved from 34.63143 to 33.08083, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-34.hdf5\n",
      "Epoch 35/100\n",
      "2920/2920 [==============================] - 1386s 475ms/step - loss: 14.4418 - mean_absolute_error: 2.8438 - val_loss: 31.9853 - val_mean_absolute_error: 4.2260\n",
      "\n",
      "Epoch 00035: val_loss improved from 33.08083 to 31.98535, saving model to ./cnn_checkpoints_gdicnn_lstm/weights-35.hdf5\n",
      "Epoch 36/100\n",
      "2920/2920 [==============================] - 1364s 467ms/step - loss: 14.5779 - mean_absolute_error: 2.8517 - val_loss: 33.6520 - val_mean_absolute_error: 4.4372\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 31.98535\n",
      "Epoch 37/100\n",
      "2920/2920 [==============================] - 1365s 467ms/step - loss: 12.8358 - mean_absolute_error: 2.6626 - val_loss: 32.3984 - val_mean_absolute_error: 4.2651\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 31.98535\n",
      "Epoch 38/100\n",
      "2920/2920 [==============================] - 1370s 469ms/step - loss: 12.2390 - mean_absolute_error: 2.6417 - val_loss: 32.1814 - val_mean_absolute_error: 4.2074\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 31.98535\n",
      "Epoch 39/100\n",
      "2920/2920 [==============================] - 1368s 469ms/step - loss: 11.4388 - mean_absolute_error: 2.5547 - val_loss: 33.4893 - val_mean_absolute_error: 4.3744\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 31.98535\n",
      "Epoch 40/100\n",
      "2696/2920 [==========================>...] - ETA: 1:39 - loss: 9.6737 - mean_absolute_error: 2.3303"
     ]
    }
   ],
   "source": [
    "# Train Model!\n",
    "\n",
    "history = video_model.fit(train_videos, train_videos_labels,callbacks=[checkpoint,lr,TerminateOnNaN(),tensorboard],\n",
    "          validation_data=(validation_videos,validation_videos_labels),\n",
    "          batch_size=batch_size, epochs=epochs,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history.history.keys())\n",
    "# summarize history for accuracy\n",
    "plt.plot(history.history['mean_absolute_error'])\n",
    "plt.plot(history.history['val_mean_absolute_error'])\n",
    "plt.title('model mean_absolute_error')\n",
    "plt.ylabel('mean_absolute_error')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()\n",
    "# summarize history for loss\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Predictions and Report Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 5, 480, 640, 3)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_videos[0:0,:,:,:,:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "269"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True=80.44922366806699, Predicted=[]\n"
     ]
    },
    {
     "ename": "FailedPreconditionError",
     "evalue": "Attempting to use uninitialized value lstm_6/kernel\n\t [[{{node lstm_6/kernel/read}} = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_6/kernel)]]\n\t [[{{node dense_12/BiasAdd/_15}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_593_dense_12/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFailedPreconditionError\u001b[0m                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-82-8fa5a952608a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# show the inputs and predicted outputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_videos\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"True=%s, Predicted=%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mvalidation_videos_labels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvideo_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_videos\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mE:\\MY\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m   1165\u001b[0m                                             \u001b[0mbatch_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1166\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1167\u001b[1;33m                                             steps=steps)\n\u001b[0m\u001b[0;32m   1168\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1169\u001b[0m     def train_on_batch(self, x, y,\n",
      "\u001b[1;32mE:\\MY\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[1;34m(model, f, ins, batch_size, verbose, steps)\u001b[0m\n\u001b[0;32m    292\u001b[0m                 \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 294\u001b[1;33m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    295\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    296\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\MY\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\MY\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\MY\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\MY\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFailedPreconditionError\u001b[0m: Attempting to use uninitialized value lstm_6/kernel\n\t [[{{node lstm_6/kernel/read}} = Identity[T=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](lstm_6/kernel)]]\n\t [[{{node dense_12/BiasAdd/_15}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_593_dense_12/BiasAdd\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]"
     ]
    }
   ],
   "source": [
    "#new = video_model.predict(validation_videos)\n",
    "# show the inputs and predicted outputs\n",
    "for i in range(len(validation_videos)):\n",
    "    print(\"True=%s, Predicted=%s\" % (validation_videos_labels[i], video_model.predict(validation_videos[0:i,:,:,:,:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(conv_dim, kernel_size=kernel_size, input_shape=input_shape, padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=2))\n",
    "# model.add(Dropout(dropout_amount))\n",
    "# model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=2))\n",
    "# model.add(Dropout(dropout_amount))\n",
    "# model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=3))\n",
    "# model.add(Dropout(dropout_amount))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(last_layer_dim,activation='relu'))\n",
    "# model.add(Dense(18, activation='softmax'))\n",
    "\n",
    "\n",
    "# from skimage import data, color\n",
    "# from skimage.transform import rescale, resize, downscale_local_mean\n",
    "# arr = train_imgs[0]\n",
    "# #arr_resize = scipy.misc.imresize(arr,(224,224,3))\n",
    "# image_resized = resize(arr, (224,224,3), anti_aliasing=false)\n",
    "\n",
    "# print('train images shape : ',train_imgs.shape)\n",
    "# print('validation images shape : ',validation_imgs.shape)\n",
    "\n",
    "# # RESHAPE FOR VGG16\n",
    "# train_imgs_crops_res = train_imgs[:,16::2, 50:274, :]\n",
    "# train_imgs_crops_res = train_imgs_crops_res[:,0:224, :, :]\n",
    "# validation_imgs_crops_res = validation_imgs[:,16::2, 50:274, :]\n",
    "# validation_imgs_crops_res = validation_imgs_crops_res[:,0:224, :, :]\n",
    "\n",
    "# print(train_imgs_crops_res.shape)\n",
    "# print(validation_imgs_crops_res.shape)\n",
    "\n",
    "# print(train_videos.shape)\n",
    "# print(train_videos_labels.shape)\n",
    "# # validation_videos=videos[~msk]\n",
    "# validation_videos_labels=labels[~msk]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
