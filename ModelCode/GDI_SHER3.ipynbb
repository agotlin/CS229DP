{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specs for running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_to_run_script = 'Sherlock' # 'Sherlock', 'local'\n",
    "spatial_component = 'vgg16' # 'vgg16', 'ANet', 'dummy'\n",
    "temporal_component = 'lstm' # '1dconv_v1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "np.random.seed(1)\n",
    "rn.seed(1)\n",
    "from keras import backend as K\n",
    "tf.set_random_seed(1)\n",
    "sess = tf.Session(graph=tf.get_default_graph())\n",
    "K.set_session(sess)\n",
    "import sys\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Activation, Dropout, Flatten\n",
    "from keras.layers import Conv1D,MaxPooling1D,Conv2D,MaxPooling2D\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Input, LSTM, Embedding, Dense\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.optimizers import SGD\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.callbacks import TerminateOnNaN\n",
    "import keras.regularizers\n",
    "import scipy\n",
    "import math\n",
    "import sys\n",
    "import pandas as pd\n",
    "from scipy.ndimage.filters import gaussian_filter1d\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import linregress\n",
    "from scipy import interpolate\n",
    "from scipy import signal\n",
    "import collections\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from PIL import Image\n",
    "import matplotlib.pylab as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions to load data\n",
    "\n",
    "def ImportImage(filepath):\n",
    "    #Check if file path exists\n",
    "    img = Image.open(filepath)\n",
    "    return np.array(img)\n",
    "\n",
    "def windows(data, size, sample_stride): # define time windows to create each training example\n",
    "    start = 0\n",
    "    while start < data.count():\n",
    "        yield int(start), int(start + size)\n",
    "        start += sample_stride\n",
    "\n",
    "def hollywood(data_full, input_window_size, sample_stride): # make video files \n",
    "    list_of_examples = []\n",
    "    labels = np.empty((0))\n",
    "    for (start, end) in windows(data_full['rownum'], input_window_size, sample_stride):   \n",
    "        if(end < data_full.shape[0] and # we are not at the end of the total frames\n",
    "            len(data_full['rownum'][start:end]) == input_window_size and  # not sure\n",
    "            int(imagepaths_wlabels['image_path'][start][21:29])==int(imagepaths_wlabels['image_path'][end-1][21:29])):  # the end patientID = start patiendID\n",
    "            # Pull ten frames, crop the images, stack them\n",
    "            ten_frames = np.array([ImportImage(img) for img in data_full['image_path'][start:end].values])\n",
    "            if spatial_component == 'vgg16': # crop images to fit into vgg16 model architecture\n",
    "                ten_frames_crops_res = ten_frames[:,16::2, 50:274, :]\n",
    "                ten_frames_crops_res = ten_frames_crops_res[:,0:224, :, :]\n",
    "                list_of_examples.append(ten_frames_crops_res)\n",
    "            else:\n",
    "                list_of_examples.append(ten_frames)\n",
    "            labels = np.append(labels,data_full['labels'][start])          \n",
    "            \n",
    "    return np.array(list_of_examples), labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull in path file\n",
    "if machine_to_run_script == 'local':\n",
    "    imagepaths_wlabels = pd.read_csv('./data/imagepath_gdi_allframes_205.csv') # TO USE REGRESSION\n",
    "elif machine_to_run_script == 'Sherlock':\n",
    "    imagepaths_wlabels = pd.read_csv('../SherlockDataFiles/imagepath_gdi_allframes_205.csv') # TO USE REGRESSION\n",
    "\n",
    "# Create video outputs with labels\n",
    "frames_per_video = 5\n",
    "stride_per_video = 50\n",
    "videos, labels = hollywood(imagepaths_wlabels, frames_per_video, stride_per_video)\n",
    "\n",
    "print(videos.shape)\n",
    "\n",
    "# Normalize data\n",
    "videos_normalized = videos # (videos - videos.mean())/videos.std()\n",
    "\n",
    "# Split data into train and validation\n",
    "msk = np.random.rand(len(videos_normalized)) < 0.8\n",
    "train_videos=videos_normalized[msk]\n",
    "train_videos_labels=labels[msk]\n",
    "validation_videos=videos_normalized[~msk]\n",
    "validation_videos_labels=labels[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_videos=videos_normalized[msk]\n",
    "train_videos_labels=labels[msk]\n",
    "validation_videos=videos_normalized[~msk]\n",
    "validation_videos_labels=labels[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# videos_normalized = videos # (videos - videos.mean())/videos.std()\n",
    "\n",
    "# # Split data into train and validation\n",
    "# msk = np.random.rand(len(videos_normalized)) < 0.8\n",
    "# train_videos=videos_normalized[msk]\n",
    "# train_videos_labels=labels[msk]\n",
    "# validation_videos=videos_normalized[~msk]\n",
    "# validation_videos_labels=labels[~msk]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(train_videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check expected shapes and whether data is populated with zeros\n",
    "\n",
    "print('train images shape : ',train_videos.shape)\n",
    "print('train labels shape : ',train_videos_labels.shape)\n",
    "print('validation images shape : ',validation_videos.shape)\n",
    "print('validation labels shape : ',validation_videos_labels.shape)\n",
    "print(train_videos[0,1,10:15,15:20,1]) # show a random rectangle of one channel of the the image\n",
    "\n",
    "# Check if any images are zeros\n",
    "mask_t = np.ones(len(train_videos), dtype=bool)\n",
    "for i, video in enumerate(train_videos):\n",
    "    if (np.count_nonzero(video)<1):\n",
    "        #print('all zeros at : ', i)\n",
    "        np.delete(train_videos,i)\n",
    "        np.delete(train_videos_labels,i)\n",
    "        mask_t[i] = False\n",
    "\n",
    "train_videos = train_videos[mask_t,...]\n",
    "train_videos_labels = train_videos_labels[mask_t,...]\n",
    "\n",
    "mask_v = np.ones(len(train_videos), dtype=bool)\n",
    "for i, video in enumerate(validation_videos):\n",
    "    if (np.count_nonzero(video)<1):\n",
    "        #print('all zeros at : ', i)\n",
    "        np.delete(validation_videos,i)\n",
    "        np.delete(validation_videos_labels,i)\n",
    "        mask_v = np.ones(len(validation_videos), dtype=bool)\n",
    "        mask_v[i] = False\n",
    "        \n",
    "validation_videos = validation_videos[mask_v,...]\n",
    "validation_videos_labels = validation_videos_labels[mask_v,...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train images shape : ',train_videos.shape)\n",
    "print('train labels shape : ',train_videos_labels.shape)\n",
    "print('validation images shape : ',validation_videos.shape)\n",
    "print('validation labels shape : ',validation_videos_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial Component of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Parameters\n",
    "\n",
    "checkpoint_folder = \"./cnn_checkpoints_gdicnn\"\n",
    "epochs = 50\n",
    "epochs_drop,drop_factor = (5,0.8)\n",
    "batch_size  = 16\n",
    "video_shape = train_videos[0].shape\n",
    "frame_shape = train_videos[0][0].shape\n",
    "kernel_size = 8\n",
    "conv_dim = 16 # number of kernels\n",
    "initial_lrate = 0.01\n",
    "dropout_amount = 0.5\n",
    "l2_lambda = 10**(-3.5)\n",
    "reg = keras.regularizers.l2(l2_lambda)\n",
    "\n",
    "def step_decay(initial_lrate,epochs_drop,drop_factor):\n",
    "    def step_decay_fcn(epoch):\n",
    "        return initial_lrate * math.pow(drop_factor, math.floor((1+epoch)/epochs_drop))\n",
    "    return step_decay_fcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if spatial_component == 'vgg16':\n",
    "\n",
    "    # Configure VGG Model:\n",
    "    vgg16_model = keras.applications.vgg16.VGG16() # Download Model\n",
    "    type(vgg16_model) #This is a Keras Functional API need to convert to sequential\n",
    "    Frame_model = Sequential() #Iterate over the functional layers and add it as a stack\n",
    "    for layer in vgg16_model.layers:\n",
    "        Frame_model.add(layer)\n",
    "\n",
    "    # Remove last layer of VGG:\n",
    "    Frame_model.layers.pop()\n",
    "\n",
    "    # Fix the VGG model\n",
    "    for layer in Frame_model.layers: #Since the model is already trained with certain weights, we dont want to change it. Let it be the same\n",
    "        layer.trainable = False\n",
    "\n",
    "    # Add a Dense layer to VGG\n",
    "    Frame_model.add(Dense(16, activation='relu')) # Add a connected layer\n",
    "    \n",
    "    print(Frame_model.summary())\n",
    "    \n",
    "\n",
    "elif spatial_component == 'dummy':\n",
    "    \n",
    "    Frame_model = Sequential()\n",
    "    Frame_model.add(Conv2D(conv_dim, kernel_size=kernel_size, input_shape=frame_shape, padding='valid', strides = 5))\n",
    "    Frame_model.add(Activation('relu'))\n",
    "    Frame_model.add(BatchNormalization())\n",
    "    Frame_model.add(Flatten())\n",
    "    Frame_model.add(Dense(18,activation='relu'))\n",
    "\n",
    "elif spatial_component == 'ANet':\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(conv_dim, kernel_size=kernel_size, input_shape=input_shape, padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(dropout_amount))\n",
    "    model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=2))\n",
    "    model.add(Dropout(dropout_amount))\n",
    "    model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D(pool_size=3))\n",
    "    model.add(Dropout(dropout_amount))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(last_layer_dim,activation='relu'))\n",
    "    model.add(Dense(1, activation='linear'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if temporal_component == 'lstm':\n",
    "    \n",
    "    # Declare video_model and apply frame_model to each frame\n",
    "    video_input = Input(shape=video_shape) # usually 10 frames that are 224x224x3 each\n",
    "    encoded_frame_sequence = TimeDistributed(Frame_model)(video_input) # Run frame_model on each frame\n",
    "\n",
    "    # Add LSTM                  \n",
    "    encoded_video = LSTM(256)(encoded_frame_sequence)  # the output will be a vector\n",
    "\n",
    "    # Add a linear layer for output of video model\n",
    "    output = Dense(1, activation='linear')(encoded_video)\n",
    "\n",
    "    # Configure video_model\n",
    "    video_model = Model(inputs=video_input, outputs=output)   \n",
    "    \n",
    "    video_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile Model and Train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compile model\n",
    "\n",
    "if not os.path.exists(checkpoint_folder):\n",
    "    os.makedirs(checkpoint_folder)\n",
    "\n",
    "filepath=checkpoint_folder+\"/weights-{epoch:02d}.hdf5\"\n",
    "\n",
    "# Create optimizer\n",
    "opt = keras.optimizers.Adam(lr=0.01, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n",
    "\n",
    "# Declare optimizer, lsos function, and reporting metrics\n",
    "video_model.compile(loss='mse',metrics=['mae'],optimizer=opt)\n",
    "\n",
    "# Create checkpoints to keep track of weights\n",
    "checkpoint = \\\n",
    "    ModelCheckpoint(filepath, verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)\n",
    "\n",
    "# Define learning rate function\n",
    "lr = LearningRateScheduler(step_decay(initial_lrate,epochs_drop,drop_factor))\n",
    "\n",
    "# Add tensorboard logs\n",
    "#tensorboard = keras.callbacks.TensorBoard(log_dir='./tensorboard-logs/{}'.format(time()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Model!\n",
    "\n",
    "history = video_model.fit(train_videos, train_videos_labels,callbacks=[checkpoint,lr,TerminateOnNaN()], #,tensorboard],\n",
    "          validation_data=(validation_videos,validation_videos_labels),\n",
    "          batch_size=batch_size, epochs=epochs,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Predictions and Report Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ynew = video_model.predict(validation_videos)\n",
    "# show the inputs and predicted outputs\n",
    "for i in range(len(validation_videos)):\n",
    "    print(\"True=%s, Predicted=%s\" % (validation_videos_labels[i], Ynew[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "# model.add(Conv2D(conv_dim, kernel_size=kernel_size, input_shape=input_shape, padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same'))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=2))\n",
    "# model.add(Dropout(dropout_amount))\n",
    "# model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=2))\n",
    "# model.add(Dropout(dropout_amount))\n",
    "# model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(Conv2D(conv_dim,kernel_size=kernel_size,padding='same',kernel_regularizer=reg))\n",
    "# model.add(Activation('relu'))\n",
    "# model.add(BatchNormalization())\n",
    "# model.add(MaxPooling2D(pool_size=3))\n",
    "# model.add(Dropout(dropout_amount))\n",
    "# model.add(Flatten())\n",
    "# model.add(Dense(last_layer_dim,activation='relu'))\n",
    "# model.add(Dense(18, activation='softmax'))\n",
    "\n",
    "\n",
    "# from skimage import data, color\n",
    "# from skimage.transform import rescale, resize, downscale_local_mean\n",
    "# arr = train_imgs[0]\n",
    "# #arr_resize = scipy.misc.imresize(arr,(224,224,3))\n",
    "# image_resized = resize(arr, (224,224,3), anti_aliasing=false)\n",
    "\n",
    "# print('train images shape : ',train_imgs.shape)\n",
    "# print('validation images shape : ',validation_imgs.shape)\n",
    "\n",
    "# # RESHAPE FOR VGG16\n",
    "# train_imgs_crops_res = train_imgs[:,16::2, 50:274, :]\n",
    "# train_imgs_crops_res = train_imgs_crops_res[:,0:224, :, :]\n",
    "# validation_imgs_crops_res = validation_imgs[:,16::2, 50:274, :]\n",
    "# validation_imgs_crops_res = validation_imgs_crops_res[:,0:224, :, :]\n",
    "\n",
    "# print(train_imgs_crops_res.shape)\n",
    "# print(validation_imgs_crops_res.shape)\n",
    "\n",
    "# print(train_videos.shape)\n",
    "# print(train_videos_labels.shape)\n",
    "# # validation_videos=videos[~msk]\n",
    "# validation_videos_labels=labels[~msk]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
